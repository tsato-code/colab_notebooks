{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n- Kaggle ã®2021å¹´10æœˆã® playground ã® EDA ã¨äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä½œæˆã€‚\n- logging\n- EDA\n    - åŸºæœ¬çš„ãªçµ±è¨ˆé‡\n    - è€ƒãˆæ–¹ï¼š\n        - train_x ã¨ train_y ã® \"ç›¸é–¢\"\n        - train_x ã¨ test_x ã®åˆ†å¸ƒã®å·®ã®æ¤œå®š\n    - target ã®åã‚Š\n- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n    - çµ±è¨ˆé‡: min, max, mean, std, bin count\n    - å¤‰æ›ï¼š log, power, normalization\n    - æ¬¡å…ƒå‰Šæ¸›ï¼šk-means, PCA\n    - é›†ç´„ï¼š `df.groupby(['cat_column']).agg({'continuous_column': ['min', 'max', 'mean', 'std'])\n- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n    - åºç›¤ã¯ãƒã‚¤ãƒ‘ãƒ©èª¿æ•´ã—ãªã„ã€‚\n    - LightGBM Tuner ã§è‡ªå‹•ãƒã‚¤ãƒ‘ãƒ©èª¿æ•´ã€‚\n    - ç´°ã‹ãã‚„ã‚‹ãªã‚‰è‡ªå‰ã§ãƒã‚¤ãƒ‘ãƒ©èª¿æ•´ã€‚\n- äº¤å·®æ¤œè¨¼\n    - 5-fold CV ã«ã‚ˆã‚‹äºˆæ¸¬çµæœã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€‚\n\nTODO\n- EDA\n    - Theils'U\n- LightGBM\n    - è¨ˆç®—ãŒçµ‚ã‚ã‚‰ãªã„ >4h\n- XGBoost\n    - [[TPS Oct 21] EDA & Modeling ğŸ”¥|Kaggle](https://www.kaggle.com/vishwas21/tps-oct-21-eda-modeling)\n\n# References\n- [Tabular Playground Series - Oct 2021](https://www.kaggle.com/c/tabular-playground-series-oct-2021)\n- [[TPS-Oct] Simple EDA | Kaggle](https://www.kaggle.com/subinium/tps-oct-simple-eda)\n- [TPS Oct 2021: single LightGBM | Kaggle](https://www.kaggle.com/hiro5299834/tps-oct-2021-single-lightgbm)\n- [TPS September 2021 EDA | Kaggle](https://www.kaggle.com/dwin183287/tps-september-2021-eda)\n- ã‚¹ãƒ¢ãƒ¼ãƒ«ã‚µãƒ³ãƒ—ãƒ«ï¼ˆç´„7000ï¼‰[TPS July 2021 EDA | Kaggle](https://www.kaggle.com/dwin183287/tps-july-2021-eda)\n- [Kaggleã§å¤šãã®å®Ÿé¨“ã‚’å›ã™ãŸã‚ã«ã‚„ã£ã¦ã„ã‚‹ç°¡å˜ãªã“ã¨ | Zenn](https://zenn.dev/fkubota/articles/2b8d46b11c178ac2fa2d)\n- [kaggle logger introduction | Kaggle](https://www.kaggle.com/sishihara/kaggle-logger-introduction)\n- [Noob Stacking | 0.85654 | Kaggle](https://www.kaggle.com/ankitkalauni/noob-stacking-0-85654/notebook)\n    - stacking ã«ç”¨ã„ã‚‹12ç¨®ã®ç‰¹å¾´é‡ãŒã»ã¼åŒä¸€ã®åˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºã‹ã‚ã¦ã„ã‚‹ã€‚\n    - ãã‚Œã‚‰12åˆ—ã®ç›¸é–¢ãŒã‹ãªã‚Šé«˜ã„ã€‚ç´„0.99\n    - ãã‚Œã‚‰12åˆ—ã‚’ä½¿ã£ã¦å­¦ç¿’ã™ã‚‹ã¨ã€äºˆæ¸¬å€¤ã®åˆ†å¸ƒã‚‚ã€ãã‚Œã‚‰12åˆ—ã«ä¸€è‡´\n    - KDE plot ã¨ç›¸é–¢ä¿‚æ•°ã® heatmap ã ã‘ä½¿ã£ãŸã¨ã®ã“ã¨\n- [How to Work with Million-row Datasets Like a Pro | towards data science](https://towardsdatascience.com/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd)\n    - æ•°ç™¾ä¸‡è¡Œã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã« datatable, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®é–¢æ•°ã€ãƒ‡ãƒ¼ã‚¿æ“ä½œã« datatable, cuDF, Dak, Vaex ãªã©ã€‚\n    - Cython, numba.jit ã‚’ä½¿ãˆã€CSV ã‚ˆã‚Šã‚‚ feather, parquet, jay ã‚’ä½¿ãˆã€ãªã©ã€‚","metadata":{}},{"cell_type":"markdown","source":"# Directory","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.751485Z","iopub.execute_input":"2021-11-10T13:19:03.751741Z","iopub.status.idle":"2021-11-10T13:19:03.761110Z","shell.execute_reply.started":"2021-11-10T13:19:03.751712Z","shell.execute_reply":"2021-11-10T13:19:03.760174Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"DEBUG_FLAG = False\nCATBOOST_FLAG = True\nVERSION = 'nb01'\n\nSUBMISSION_PATH = '/kaggle/input/tabular-playground-series-oct-2021/sample_submission.csv'\nTRAIN_PATH = '/kaggle/input/tabular-playground-series-oct-2021/train.csv'\nTEST_PATH = '/kaggle/input/tabular-playground-series-oct-2021/test.csv'\n\nN_SPLITS = 2 if DEBUG_FLAG else 5\nN_ESTIMATORS = 10 if DEBUG_FLAG else 200\nEARLY_STOPPING_ROUNDS = 3 if DEBUG_FLAG else 20","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.762995Z","iopub.execute_input":"2021-11-10T13:19:03.763599Z","iopub.status.idle":"2021-11-10T13:19:03.770307Z","shell.execute_reply.started":"2021-11-10T13:19:03.763562Z","shell.execute_reply":"2021-11-10T13:19:03.769250Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Install & import","metadata":{}},{"cell_type":"code","source":"import datetime\nimport pickle\nimport random\nimport sys\nimport time\n\nimport datatable as dt\nimport lightgbm as lgb\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport scipy.stats as ss\nimport seaborn as sns\n\nfrom catboost import CatBoostClassifier\nfrom contextlib import contextmanager\nfrom logging import getLogger, Formatter, FileHandler, StreamHandler, INFO, DEBUG\n# from optuna.integration import lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.metrics import confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\n\n# settings\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', None)\n\n# mpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T13:19:03.772254Z","iopub.execute_input":"2021-11-10T13:19:03.772805Z","iopub.status.idle":"2021-11-10T13:19:03.782550Z","shell.execute_reply.started":"2021-11-10T13:19:03.772771Z","shell.execute_reply":"2021-11-10T13:19:03.781804Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\n\ndef create_logger(exp_version):\n    log_file = (f'{exp_version}.log')\n\n    logger_ = getLogger(exp_version)\n    logger_.setLevel(DEBUG)\n\n    # formatter\n    # fmr = Formatter('[%(levelname)s] %(asctime)s >>\\t%(message)s')\n    fmr = Formatter(\n        '%(asctime)s %(name)s %(lineno)d'\n        ' [%(levelname)s][%(funcName)s] %(message)s'\n    )\n\n    # file handler\n    fh = FileHandler(log_file)\n    fh.setLevel(DEBUG)\n    fh.setFormatter(fmr)\n\n    # stream handler\n    ch = StreamHandler()\n    ch.setLevel(INFO)\n    ch.setFormatter(fmr)\n\n    logger_.addHandler(fh)\n    logger_.addHandler(ch)\n\n\ndef get_logger(exp_version):\n    return getLogger(exp_version)\n\n\ndef get_args_of_func(f):\n    return f.__code__.co_varnames[:f.__code__.co_argcount]\n\n\ndef show_mem_usage():\n    print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n    print(\" ------------------------------------ \")\n    for var_name in globals():\n        if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 1024**2:\n            print(\"{}{: >25}{}{: >6} MiB{}\".format('|',var_name,'|', int(sys.getsizeof(eval(var_name))/1024**2),'|'))\n\n\ndef read_data(train_only=True):\n    train = dt.fread(TRAIN_PATH).to_pandas()\n    if train_only:\n        test = pd.DataFrame()\n    else:\n        test = dt.fread(TEST_PATH).to_pandas()\n    return train, test\n\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\n\ndef _fe_kmeans(df, n_clusters=6, km=None):\n    cluster_cols = [f'kmeans_{i}' for i in range(n_clusters)]\n    if km is None:\n        km = KMeans(n_clusters=n_clusters, \n                    n_init=50, \n                    max_iter=500, \n                    random_state=0)\n        df_km = km.fit_transform(df)\n    else:\n        df_km = km.transform(df)\n    df_km = pd.DataFrame(df_km, columns=cluster_cols, index=df.index)\n    return df_km, km\n\n\ndef _fe_pca(df, dim=6, pca=None):\n    if pca is None:\n        pca = PCA()\n        df_pca = pca.fit_transform(df[features])\n    else:\n        df_pca = pca.transform(df[features])\n    pca_cols = [f'pca_{i}' for i in range(df_pca.shape[1])]\n    df_pca = pd.DataFrame(df_pca, columns=pca_cols, index=df.index)\n    return df_pca.iloc[:, :dim], pca\n\n\ndef _feature_engineering(df):\n    df['mean'] = df[cont_features].mean(axis=1)\n    df['std'] = df[cont_features].std(axis=1)\n    df['min'] = df[cont_features].min(axis=1)\n    df['max'] = df[cont_features].max(axis=1)\n    df['bin_count'] = df[disc_features].sum(axis=1)\n    return df\n\n\ndef feature_engineering(df, scaler=None, km=None, pca=None):\n    df = _feature_engineering(df)\n    \n    if scaler is None:\n        scaler = RobustScaler()\n        df.loc[:, ['std', 'bin_count']] = scaler.fit_transform(df[['std', 'bin_count']])\n    else:\n        df.loc[:, ['std', 'bin_count']] = scaler.transform(df[['std', 'bin_count']])\n\n    df_km, km = _fe_kmeans(df[features], km=km)\n    df_pca, pca = _fe_pca(df[features], pca=pca)\n    df = pd.concat([df, df_km, df_pca], axis=1)\n    \n    return df, scaler, km, pca","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.785194Z","iopub.execute_input":"2021-11-10T13:19:03.785492Z","iopub.status.idle":"2021-11-10T13:19:03.828815Z","shell.execute_reply.started":"2021-11-10T13:19:03.785467Z","shell.execute_reply":"2021-11-10T13:19:03.828097Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Preparing","metadata":{}},{"cell_type":"code","source":"create_logger(VERSION)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.933495Z","iopub.execute_input":"2021-11-10T13:19:03.933821Z","iopub.status.idle":"2021-11-10T13:19:03.938809Z","shell.execute_reply.started":"2021-11-10T13:19:03.933796Z","shell.execute_reply":"2021-11-10T13:19:03.937845Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Dataset overview","metadata":{}},{"cell_type":"code","source":"train, test = read_data(train_only=False)\n\nif not DEBUG_FLAG:\n    train = reduce_memory_usage(train)\n    test = reduce_memory_usage(test) \n\nif DEBUG_FLAG:\n    train = train.sample(n=10000)\n    if len(test) != 0:\n        test = test.sample(n=10000)\n\n    \nget_logger(VERSION).info(f'train.shape: {train.shape}')\nget_logger(VERSION).info(f'test.shape: {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.940902Z","iopub.execute_input":"2021-11-10T13:19:03.941925Z","iopub.status.idle":"2021-11-10T13:19:15.486401Z","shell.execute_reply.started":"2021-11-10T13:19:03.941890Z","shell.execute_reply":"2021-11-10T13:19:15.485697Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"get_logger(VERSION).info('train data')\nget_logger(VERSION).info(train.head(1))\n\nget_logger(VERSION).info('test data')\nget_logger(VERSION).info(test.head(1))\n\nget_logger(VERSION).info(f'number of missing values in train: {train.isnull().sum().sum()}')\nget_logger(VERSION).info(f'number of missing values in test: {test.isnull().sum().sum()}')\n\ntarget_count = train['target'].value_counts()\nget_logger(VERSION).info(f'target count below:')\nget_logger(VERSION).info(target_count)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:15.487685Z","iopub.execute_input":"2021-11-10T13:19:15.488369Z","iopub.status.idle":"2021-11-10T13:19:16.278000Z","shell.execute_reply.started":"2021-11-10T13:19:15.488342Z","shell.execute_reply":"2021-11-10T13:19:16.277264Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    plt.pie(target_count, \n        labels=target_count.index,\n        colors=['#76D7C4', '#F5B7B1'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.279280Z","iopub.execute_input":"2021-11-10T13:19:16.279672Z","iopub.status.idle":"2021-11-10T13:19:16.353418Z","shell.execute_reply.started":"2021-11-10T13:19:16.279638Z","shell.execute_reply":"2021-11-10T13:19:16.352765Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    display(train.loc[:, 'f0':'f284'].describe().T.style.bar(subset=['mean'], color='#205ff2') \\\n        .background_gradient(subset=['std'], cmap='Reds') \\\n        .background_gradient(subset=['50%'], cmap='coolwarm'))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.355501Z","iopub.execute_input":"2021-11-10T13:19:16.356177Z","iopub.status.idle":"2021-11-10T13:19:16.983045Z","shell.execute_reply.started":"2021-11-10T13:19:16.356141Z","shell.execute_reply":"2021-11-10T13:19:16.982262Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train.columns if col not in ['id', 'target']]\n\ncont_features = []\ndisc_features = []\n\nfor col in features:\n    if np.issubdtype(train[col].dtype, np.floating):\n        cont_features.append(col)\n    else:\n        disc_features.append(col)\n\nget_logger(VERSION).info('cont. features below:')\nget_logger(VERSION).info(cont_features)\nget_logger(VERSION).info('disc. features below:')\nget_logger(VERSION).info(disc_features)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.984597Z","iopub.execute_input":"2021-11-10T13:19:16.985070Z","iopub.status.idle":"2021-11-10T13:19:17.022949Z","shell.execute_reply.started":"2021-11-10T13:19:16.985035Z","shell.execute_reply":"2021-11-10T13:19:17.022152Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Correlation\n\n||continuous data|ordinal data|categorical data|\n|-:|:-|:-|:-|\n|continuous data|Pearson correlation coefficient|polyserial correlation coefficient|correlation ratio|\n|ordinal data||Spearman's rank correlation coefficient, <br>Kendall rank correlation coefficient, <br>Polychoric correlation coefficient|(rank correlation ratio?)|\n|categorical data|||four-fold point correlation coefficient (phi coefficient), <br>CramÃ©r's V, <br>Uncertainty coefficient (Theil's U)|\n\n[Ref.]\n- [5.5 å„ç¨®æ‰‹æ³•ã®ç›¸äº’é–¢ä¿‚ | çµ±è¨ˆå­¦å…¥é–€](http://www.snap-tck.com/room04/c01/stat/stat05/stat0505.html)","metadata":{}},{"cell_type":"code","source":"if DEBUG_FLAG:\n    cramers_v_scores = []\n    for f in disc_features:\n        cramers_v_scores.append((f, cramers_v(train['target'], train[f])))\n\n    display(pd.DataFrame(cramers_v_scores, columns=['disc_feature', 'cramers_v']) \\\n        .sort_values('cramers_v', ascending=False) \\\n        .style.bar(subset=['cramers_v'], color='#205ff2')\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:17.024471Z","iopub.execute_input":"2021-11-10T13:19:17.024972Z","iopub.status.idle":"2021-11-10T13:19:17.606411Z","shell.execute_reply.started":"2021-11-10T13:19:17.024932Z","shell.execute_reply":"2021-11-10T13:19:17.605614Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    plt.rcParams['font.size'] = 8\n    fig, axes = plt.subplots(nrows=29, ncols=10, tight_layout=True, figsize=(18, 36))\n\n    for i, ax in enumerate(axes.ravel()):\n        if i < len(features):\n            if f'f{i}' in cont_features:\n                sns.kdeplot(train[f'f{i}'], lw=.5, alpha=.3, shade=True, ax=ax)\n                sns.kdeplot(test[f'f{i}'], lw=.5, alpha=.3, shade=True, ax=ax)\n            elif f'f{i}' in disc_features:\n                tr = train[[f'f{i}']]\n                tr = tr.assign(data = 'Train')\n                te = test[[f'f{i}']]\n                te = te.assign(data = 'Test')\n                sns.countplot(x='data', \n                              hue=f'f{i}', \n                              order=['Train', 'Test'],\n                              alpha=.3, \n                              data=pd.concat([tr, te]), \n                              ax=ax)\n        else:\n            ax.set_axis_off()\n        \n        ax.set_ylabel(None)\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:17.607817Z","iopub.execute_input":"2021-11-10T13:19:17.608682Z","iopub.status.idle":"2021-11-10T13:20:08.920319Z","shell.execute_reply.started":"2021-11-10T13:19:17.608638Z","shell.execute_reply":"2021-11-10T13:20:08.919657Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 15))\n    sns.heatmap(train[cont_features].corr(), \n                vmin=-1.0,\n                vmax=1.0,\n                annot=True, \n                fmt='1.1f', \n                cmap='coolwarm', \n                ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:20:08.921475Z","iopub.execute_input":"2021-11-10T13:20:08.922189Z","iopub.status.idle":"2021-11-10T13:23:08.444719Z","shell.execute_reply.started":"2021-11-10T13:20:08.922153Z","shell.execute_reply":"2021-11-10T13:23:08.444109Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## ç›¸äº’æƒ…å ±é‡\nãã‚Œãã‚Œã®ç‰¹å¾´é‡ã®ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬å› å­ã¨ã—ã¦ã®æœ‰ç”¨æ€§ã‚’ç¢ºèªã™ã‚‹ã€‚  \nã„ãã¤ã‹æ–¹æ³•ã¯ã‚ã‚‹ãŒã€ã²ã¨ã¤ã®æ–¹æ³•ã¨ã—ã¦ã€ç›¸äº’æƒ…å ±é‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚  \nç›¸äº’æƒ…å ±é‡ã¯ã€ç›¸é–¢ä¿‚æ•°ã®ã‚ˆã†ã«ãµãŸã¤ã®ç‰¹å¾´é‡é–“ã® \"ç›¸é–¢é–¢ä¿‚\" ã‚’å®šé‡åŒ–ã™ã‚‹ã€‚  \nç›¸é–¢ä¿‚æ•°ãŒç·šå½¢ãªé–¢ä¿‚ã«æ³¨ç›®ã—ã¦å®šé‡åŒ–ã™ã‚‹ã®ã«å¯¾ã—ã€ç›¸äº’æƒ…å ±é‡ã¯ç·šå½¢ã§ãªãã¦ã‚‚ç›¸é–¢é–¢ä¿‚ã‚’å®šé‡åŒ–ã§ãã‚‹ã€‚  \nç›¸äº’æƒ…å ±é‡ã®åˆ©ç‚¹ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šï¼š\n- ç°¡å˜ã«ä½¿ãˆã‚‹ã—ã€ç°¡å˜ã«èª¬æ˜ã§ãã‚‹\n- åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã‚‹\n- ç†è«–çš„ãªæ ¹æ‹ ãŒã—ã£ã‹ã‚Šã—ã¦ã„ã‚‹\n- éå­¦ç¿’ã«è€æ€§ãŒã‚ã‚‹ï¼ˆæœ¬å½“ã‹ï¼Ÿï¼‰\n- ä»»æ„ã®ç¨®é¡ã®é–¢ä¿‚ã‚’æ¤œå‡ºã§ãã‚‹\n\n2ã¤ã®ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹ç›¸äº’æƒ…å ±é‡ã¯ã€ä¸€æ–¹ã®ç‰¹å¾´é‡ã®å€¤ã‚’çŸ¥ã‚‹ã“ã¨ã§ã€ä»–æ–¹ã®å€¤ã®ä¸ç¢ºå®Ÿæ€§ãŒã©ã‚Œã ã‘ä½æ¸›ã™ã‚‹ã‹ã‚’ç¤ºã™å°ºåº¦ã§ã‚ã‚‹ã€‚  \nç›¸äº’æƒ…å ±é‡ã®æœ€å°ã®å€¤ã¯0ã§ã€ã“ã®å ´åˆã¯2ã¤ã®ç‰¹å¾´é‡ãŒç‹¬ç«‹ãªé–¢ä¿‚ã«ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚  \nç›¸äº’æƒ…å ±é‡ã®ä¸Šé™ã¯ãªã„ã€‚\n\nç›¸äº’æƒ…å ±é‡ã¯ã€ãã®ç‰¹å¾´é‡å˜ç‹¬ã§äºˆæ¸¬å› å­ã¨ã—ãŸã¨ãã®æœ‰ç”¨æ€§ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚  \nè¤‡æ•°ã®ç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨ã«ã‚ˆã£ã¦ã€ãã‚Œã‚‰ã®ç‰¹å¾´é‡ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®äºˆæ¸¬å› å­ã¨ã—ã¦æœ‰ç”¨ã§ã‚ã‚‹ãŒã€ãã‚Œãã‚Œå˜ç‹¬ã§ã¯æœ‰ç”¨ã§ã¯ãªã„ã“ã¨ãŒã‚ã‚‹ã®ã§ã€ç›¸äº’æƒ…å ±é‡ã«æ„å‘³ãŒã‚ã‚‹ã€‚\n\n<br><br>\nå®šç¾©ã¨ä¾‹ã‚’ç¢ºèªã™ã‚‹ã€‚  \nãµãŸã¤ã®é›¢æ•£ç¢ºç‡å¤‰æ•°Xã¨Yã®ç›¸äº’æƒ…å ±é‡ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼š\n$$\nI(X, Y)=\\sum_{y\\in Y}\\sum_{x\\in X}p(x, y)\\log\\frac{p(x, y)}{p(x)p(y)}.\n$$\nã“ã“ã§$p(x, y)$ã¯$X$ã¨$Y$ã®åŒæ™‚åˆ†å¸ƒé–¢æ•°ã€$p(x)$ã¨$p(y)$ã¯ãã‚Œãã‚Œ$X$ã¨$Y$ã®å‘¨è¾ºåˆ†å¸ƒé–¢æ•°ã§ã‚ã‚‹ã€‚\n\nä»¥ä¸‹ã¯ç¢ºç‡å¤‰æ•°Xã¨YãŒã€ã©ã¡ã‚‰ã‚‚å–ã‚Šã†ã‚‹å€¤ãŒ$\\{0, 1\\}$ã®å ´åˆã®ã‚¯ãƒ­ã‚¹è¡¨ã€‚\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |$a_{00}$|$a_{01}$|\n| X=1  |$a_{10}$|$a_{11}$|\n\nç·æ•°ã‚’$N (=a_{00}+a_{01}+a_{10}+a_{11})$ã¨ã™ã‚‹ã¨ã€$p(X=0, Y=0)=a_{00}/N$, $p(X=0)=(a_{00}+a_{01})/N$, $p(Y=0)=(a_{00}+a_{10})/N$ã¨ãªã‚‹ã€‚  \nã“ã®ã¨ãã®ç›¸äº’æƒ…å ±é‡ã¯\n$$\n\\begin{align}\nI(X, Y)\n= & p(X=0, Y=0)\\log\\frac{p(X=0, Y=0)}{p(X=0)p(Y=0)} \\\\\n+ & p(X=1, Y=0)\\log\\frac{p(X=1, Y=0)}{p(X=1)p(Y=0)} \\\\\n+ & p(X=0, Y=1)\\log\\frac{p(X=0, Y=1)}{p(X=0)p(Y=1)} \\\\\n+ & p(X=1, Y=1)\\log\\frac{p(X=1, Y=1)}{p(X=1)p(Y=1)} \\\\[10pt]\n= & \\frac{a_{00}}{N}\\log\\frac{a_{00}/N}{(a_{00}+a_{01})/N\\cdot (a_{00}+a_{10})/N} \\\\\n+ & \\frac{a_{10}}{N}\\log\\frac{a_{10}/N}{(a_{10}+a_{11})/N\\cdot (a_{00}+a_{10})/N} \\\\\n+ & \\frac{a_{01}}{N}\\log\\frac{a_{01}/N}{(a_{00}+a_{01})/N\\cdot (a_{01}+a_{11})/N} \\\\\n+ & \\frac{a_{11}}{N}\\log\\frac{a_{11}/N}{(a_{10}+a_{11})/N\\cdot (a_{01}+a_{11})/N}\n\\end{align}\n$$\n\næ¥µç«¯ãªä¾‹ã‚’è€ƒãˆã‚‹ã€‚ \n\n#### ä¾‹1\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |   1  |\n\nã“ã®ã¨ãã®ç›¸äº’æƒ…å ±é‡ã¯\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{2}\\log\\frac{1/2}{1/2\\cdot 1/2}\n+  \\frac{0}{2}\\log\\frac{0/2}{1/2\\cdot 1/2}\n+  \\frac{0}{2}\\log\\frac{0/2}{1/2\\cdot 1/2}\n+  \\frac{1}{2}\\log\\frac{1/2}{1/2\\cdot 1/2} \\\\[10pt]\n=& 1\n\\end{align}\n$$\n\n#### ä¾‹2\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   1  |\n| X=1  |   1  |   1  |\n\nã“ã®ã¨ãã®ç›¸äº’æƒ…å ±é‡ã¯\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4} \\\\[10pt]\n=& 0\n\\end{align}\n$$\n\n#### ä¾‹3\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |   3  |\n\nã“ã®ã¨ãã®ç›¸äº’æƒ…å ±é‡ã¯\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{4}\\log\\frac{1/4}{1/4\\cdot 1/4}\n+  \\frac{0}{4}\\log\\frac{0/4}{3/4\\cdot 1/4}\n+  \\frac{0}{4}\\log\\frac{0/4}{1/4\\cdot 3/4}\n+  \\frac{3}{4}\\log\\frac{3/4}{3/4\\cdot 3/4} \\\\[10pt]\n=& \\frac{1}{4}\\log 4 + \\frac{3}{4}\\log \\frac{4}{3} \\\\[10pt]\n\\approx& 0.8112\n\\end{align}\n$$\n\n#### ä¾‹4\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |  99  |\n\nã“ã®ã¨ãã®ç›¸äº’æƒ…å ±é‡ã¯\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{100}\\log\\frac{1/100}{1/100\\cdot 1/100}\n+  \\frac{0}{100}\\log\\frac{0/100}{99/100\\cdot 1/100}\n+  \\frac{0}{100}\\log\\frac{0/100}{1/100\\cdot 99/100}\n+  \\frac{99}{100}\\log\\frac{99/100}{99/100\\cdot 99/100} \\\\[10pt]\n=& \\frac{1}{100}\\log 100 + \\frac{99}{100}\\log \\frac{100}{99} \\\\[10pt]\n\\approx& 0.0808\n\\end{align}\n$$\n\n<br><br>\nscikit-learn ã§ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒé›¢æ•£ãƒ‡ãƒ¼ã‚¿ã‹é‡çš„ãƒ‡ãƒ¼ã‚¿ã‹ã§ã€ä½¿ã†é–¢æ•°ãŒç•°ãªã‚‹ã€‚  \né›¢æ•£ã®å ´åˆã¯ sklearn.feature_selection.mutual_info_refression ã§ã‚ã‚‹ã€‚  \né€£ç¶šã®å ´åˆã¯ sklearn.feature_selection.mutual_info_classif ã§ã‚ã‚‹ã€‚\n\n[å‚è€ƒè¨˜äº‹] \n- [Mutual Information | Kaggle](https://www.kaggle.com/ryanholbrook/mutual-information)\n- ã‚¯ãƒ­ã‚¹è¡¨ã‹ã‚‰è¨ˆç®— [ç›¸äº’æƒ…å ±é‡ã‚’ç”¨ã„ãŸç‰¹å¾´é¸æŠ | äººå·¥çŸ¥èƒ½ã«é–¢ã™ã‚‹æ–­å‰µéŒ²](https://aidiary.hatenablog.com/entry/20100619/1276950312)","metadata":{}},{"cell_type":"code","source":"if DEBUG_FLAG:\n    mi_scores = mutual_info_classif(train[features], train['target'], discrete_features=[f in disc_features for f in features])\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=features)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    \n    def plot_mi_scores(scores):\n        scores = scores.sort_values(ascending=True)\n        width = np.arange(len(scores))\n        ticks = list(scores.index)\n        plt.barh(width, scores, height=.4)\n        plt.yticks(width, ticks)\n        plt.ylim(-1, len(scores))\n        plt.title('Mutual Information Scores')\n        plt.grid()\n\n    top = 40\n    # plt.style.use('seaborn-talk')\n    plt.figure(figsize=(8, 8))\n    plot_mi_scores(mi_scores[:top])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:08.445931Z","iopub.execute_input":"2021-11-10T13:23:08.446307Z","iopub.status.idle":"2021-11-10T13:23:28.681629Z","shell.execute_reply.started":"2021-11-10T13:23:08.446239Z","shell.execute_reply":"2021-11-10T13:23:28.680957Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"show_mem_usage()\n\nif not DEBUG_FLAG and 'test' in globals():\n    del test","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.682929Z","iopub.execute_input":"2021-11-10T13:23:28.683324Z","iopub.status.idle":"2021-11-10T13:23:28.747760Z","shell.execute_reply.started":"2021-11-10T13:23:28.683287Z","shell.execute_reply":"2021-11-10T13:23:28.746996Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Model build\n\näºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚\nã“ã“ã§ã¯å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®ã²ã¨ã¤ã§ã‚ã‚‹LightGBMã‚’ç”¨ã„ã‚‹ã€‚  \nå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã¯ã€å¼±å­¦ç¿’å™¨ã‚’ã¤ãã‚Šã€ãã®èª¤å·®ã‚’å¸åã™ã‚‹ã‚ˆã†ã«å¼±å­¦ç¿’å™¨ã‚’é€æ¬¡çš„ã«ã¤ãã‚‹å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚  \nå¼±å­¦ç¿’å™¨ã«ã¯ã€æ±ºå®šæœ¨ãŒç”¨ã„ã‚‰ã‚Œã‚‹ã“ã¨ãŒå¤šãã€Gradient Boosting Decision Tree (GBDT) ã¨ã„ã†ã€‚  \nGBDTã®ãªã‹ã§ã¯ã€XGBoost, LightGBM, CatBoostãªã©ãŒæœ‰åã§ã‚ã‚‹ã€‚  \nãã‚Œã‚‰ã¯ã©ã†ç•°ãªã‚‹ã®ã‹ã€pros, consã«ã¤ã„ã¦èª¿æŸ»ã™ã‚‹ã€‚\n\n**XGBoost**  \nXGBoostã¯ã€åˆ†å‰²ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠã™ã‚‹éš›ã«ã€äº‹å‰ã‚½ãƒ¼ãƒˆã«ã‚ˆã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã«ã‚ˆã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®2ç¨®é¡ã‚’ä½¿ã†ã€‚  \näº‹å‰ã‚½ãƒ¼ãƒˆã¨ã¯ã€åˆ†å‰²ã®éš›ã«ã€ä»»æ„ã®ç‰¹å¾´é‡ã®å€¤ã«åŸºã¥ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚½ãƒ¼ãƒˆã—ã€ãã®ç‰¹å¾´é‡ã§åˆ†å‰²ã™ã‚‹éš›ã®æƒ…å ±åˆ©å¾—ã«ã‚ˆã‚‹æœ€é©ãªåˆ†å‰²å€¤ã‚’æ±ºå®šã™ã‚‹ã€‚  \nã“ã‚Œã«ã‚ˆã‚Šã€ã™ã¹ã¦ã®ç‰¹å¾´é‡ãã‚Œãã‚Œã§åˆ†å‰²ã™ã‚‹éš›ã®æœ€é©ãªåˆ†å‰²å€¤ã‚’å¾—ã‚‹ã€‚  \nãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€ãã‚Œãã‚Œã®ç‰¹å¾´é‡ã§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’æ§‹æˆã—ã€ãã®ãƒ“ãƒ³ã‚’ä½¿ã£ã¦åˆ†å‰²å€¤ã‚’æ±‚ã‚ã‚‹ã€‚  \nã“ã‚Œã‚‰2ã¤ã®åˆ†å‰²ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€é€Ÿåº¦ã®ç‚¹ã§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå„ªã‚Œã¦ã„ã‚‹ã€‚\n\n**LightGBM**  \nLightGBMã¯ã€åˆ†å‰²ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠã™ã‚‹éš›ã«ã€Gradient-based One-Side Sampling (GOSS)ã‚’ç”¨ã„ã‚‹ã€‚  \nã“ã“ã§ \"å‹¾é…\" ãŒæ„å‘³ã—ã¦ã„ã‚‹ã®ã¯ã€æå¤±é–¢æ•°ã«å¯¾ã™ã‚‹æ¥ç·šã®å‚¾ãã§ã‚ã‚‹ã€‚â– â– â– æ¥ç·šã¨ã¯ï¼Ÿè¦ç¢ºèª  \nã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«å‹¾é…ã‚’è¨ˆç®—ã—ã€å‹¾é…ãŒå¤§ãã„ã‚µãƒ³ãƒ—ãƒ«ã¯ãã®ã¾ã¾æ®‹ã—ã€å‹¾é…ãŒå°ã•ã„ã‚µãƒ³ãƒ—ãƒ«ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚  \n\n**CatBoost**  \nã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®æ‰±ã„æ–¹ã«ç‰¹å¾´ãŒã‚ã‚‹ã€‚  \nç‰¹å¾´é‡ã®ä¸€æ„ãªå€¤ã®æ•°ãŒã€ \"one_hot_max_size\" ã§æŒ‡å®šã—ãŸå€¤ä»¥ä¸‹ã®ã¨ãã« one hot encoding ã‚’è¡Œã†ã€‚  \nç‰¹å¾´é‡ã®ä¸€æ„ãªå€¤ã®æ•°ãŒã€ \"one_hot_max_size\" ã§æŒ‡å®šã—ãŸå€¤ã‚ˆã‚Šã‚‚å¤§ãã„ã¨ã Ordered TS ã¨å‘¼ã°ã‚Œã‚‹ Target Encoding ã®å‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹ã€‚\n\nã¡ãªã¿ã«ã€XGBoost ã¯ LightGBM ã‚„ CatBoost ã¨ã¯ç•°ãªã‚Šã€ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’å†…éƒ¨çš„ã«æ‰±ã†ã“ã¨ã¯ã§ããšã€äººæ‰‹ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n\n|Func.|XGBoost|CatBoost|LightGBM|\n|-:|:-|:-|:-|\n|control overfitting|learning rate or eta, <br>max_depth, <br> min_child_weight|learning_rate, <br>depth, <br>(no such feature like min_child_weight)|learning_rate, <br>max_depth, <br>num_leaves, <br>min_data_in_leaf|\n|categoriccal values|(not available)|cat_features, <br>one_hot_max_size|categorical_feature|\n|controlling speed|colsample_bytree, <br>subsample, <br>n_estimators|rsm, <br>(no such parameter to subset data), <br>iteratons|feature_fraction, <br>bagging_fraction, <br>num_iterations|\n\n[å‚è€ƒ]\n- [å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®åŸºç¤ã¨æœ€æ–°ã®å‹•å‘ (MIRU2020 Tutorial)](https://www.slideshare.net/RyuichiKanoh/miru2020-tutorial-237272385)\n- [CatBoost vs. Light GBM vs. XGBoost | towards data science](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)","metadata":{}},{"cell_type":"code","source":"?lgb.train","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.748858Z","iopub.execute_input":"2021-11-10T13:23:28.749399Z","iopub.status.idle":"2021-11-10T13:23:28.760014Z","shell.execute_reply.started":"2021-11-10T13:23:28.749359Z","shell.execute_reply":"2021-11-10T13:23:28.759206Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"models = []\nresults = []\noof_train = np.zeros((len(train),))\n\nstkf = StratifiedKFold(n_splits=N_SPLITS, \n                       shuffle=True, \n                       random_state=0)\n\nparams_lgb = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'max_depth': 12,\n    #'num_leaves': 500,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.80,\n    'bagging_fraction': 0.80,\n    'bagging_freq': 5,\n    'max_bin': 100,\n    'n_jobs':-1,\n    'verbose': -1,\n    #'num_threads': 1,\n    'lambda_l2': 1.5,\n    #'min_gain_to_split': 0,\n    # 'is_unbalance': True,\n    #'scale_pos_weight':0.15,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}  \nparams_lgb = {\n    'objective': 'binary',\n    'n_estimators': 2000,\n    'random_state': 0,\n    'learning_rate': 0.05,\n    'early_stopping_rounds': 10,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device': 'gpu',\n}\n\n\nparams_cat = {\n    'iterations': 2000, \n    'loss_function': 'Logloss', \n    'depth': 8, \n    'task_type' : 'GPU',\n    'use_best_model': True,\n    'eval_metric': 'AUC',\n    'early_stopping_rounds': 100,\n    'learning_rate': 0.05,\n    'border_count': 32,\n    'l2_leaf_reg': 3,\n    'verbose': 100\n}\n\ny_train = train['target']\nX_train = train.drop(['id', 'target'], axis=1)\n\nget_logger(VERSION).info(f'cross validation:')\n\nfor fold_id, (train_idx, valid_idx) in enumerate(stkf.split(X_train, y_train)):\n    start = time.time()\n    result = {}\n    \n    get_logger(VERSION).info(f'||' * 40)\n    get_logger(VERSION).info(f'fold_id: {fold_id}')\n    \n    X_tr = X_train.iloc[train_idx, :].reset_index(drop=True)\n    X_val = X_train.iloc[valid_idx, :].reset_index(drop=True)\n    y_tr = y_train.iloc[train_idx].reset_index(drop=True)\n    y_val = y_train.iloc[valid_idx].reset_index(drop=True)\n    \n    get_logger(VERSION).info(f'feature engineering:')\n    \n    #X_tr, scaler, km, pca = feature_engineering(X_tr)\n    #X_val, scaler, km, pca = feature_engineering(X_val, scaler, km, pca)\n    \n    get_logger(VERSION).info(' '.join(X_tr.columns))\n    \n    if CATBOOST_FLAG:\n        model_cat = CatBoostClassifier(**params_cat)\n        model_cat.fit(X_tr,\n                      y_tr,\n                      eval_set=[(X_val, y_val)],\n                      early_stopping_rounds=10)\n        \n        oof_train[valid_idx] = model_cat.predict_proba(X_val)[:,1]\n        models.append(model_cat)\n        \n    else:\n        lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=disc_features)\n        lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature=disc_features)\n\n        model_lgb = lgb.train(params_lgb,\n                              lgb_train,\n                              valid_sets=[lgb_train, lgb_eval],\n                              valid_names=['Train', 'Valid'],\n                              verbose_eval=100,\n                              num_boost_round=500,\n                              evals_result=result)\n\n        oof_train[valid_idx] = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n        models.append(model_lgb)\n        results.append(result)\n    \n    score_auc = roc_auc_score(y_val, oof_train[valid_idx])\n    elapsed = time.time() - start\n    get_logger(VERSION).info(f'fold {fold_id} - auc: {score_auc:.6f}, elapsed time: {elapsed:.2f} [sec]\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.761619Z","iopub.execute_input":"2021-11-10T13:23:28.761894Z","iopub.status.idle":"2021-11-10T13:23:42.942930Z","shell.execute_reply.started":"2021-11-10T13:23:28.761861Z","shell.execute_reply":"2021-11-10T13:23:42.942281Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nmodel_name = f'trained_model_lgb_{now}.pkl'\npickle.dump(models, open(model_name, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:42.948531Z","iopub.execute_input":"2021-11-10T13:23:42.949478Z","iopub.status.idle":"2021-11-10T13:23:42.963897Z","shell.execute_reply.started":"2021-11-10T13:23:42.949439Z","shell.execute_reply":"2021-11-10T13:23:42.963266Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"if CATBOOST_FLAG:\n    fig, axes = plt.subplots(nrows=1, ncols=len(models), tight_layout=True, figsize=(6*len(models), 4))\n    for i, model in enumerate(models):\n        history = model.get_evals_result()\n        train_metric = history['learn']['Logloss']\n        axes[i].plot(train_metric, label='train metric')\n        eval_metric = history['validation']['Logloss']\n        axes[i].plot(eval_metric, label='eval metric')\n        axes[i].set_title(f'fold {i}')\n        axes[i].set_xlabel('Iterations')\n        axes[i].set_ylabel('logloss')\nelse:\n    fig, axes = plt.subplots(nrows=1, ncols=len(results), tight_layout=True, figsize=(6*len(results), 4))\n    for i, res in enumerate(results):\n        lgb.plot_metric(res, ax=axes[i])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:42.968013Z","iopub.execute_input":"2021-11-10T13:23:42.970516Z","iopub.status.idle":"2021-11-10T13:23:43.403648Z","shell.execute_reply.started":"2021-11-10T13:23:42.970482Z","shell.execute_reply":"2021-11-10T13:23:43.403099Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"test = dt.fread(TEST_PATH).to_pandas()\ntest = test.drop(['id'], axis=1)\nif DEBUG_FLAG:\n    test = test.sample(n=10000)\n#test, _, _, _ = feature_engineering(test, scaler, km, pca)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:41:39.090280Z","iopub.execute_input":"2021-11-10T13:41:39.090563Z","iopub.status.idle":"2021-11-10T13:41:42.775156Z","shell.execute_reply.started":"2021-11-10T13:41:39.090527Z","shell.execute_reply":"2021-11-10T13:41:42.774271Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"models = pickle.load(open(model_name, 'rb'))\n\ny_preds = []\nfor model in models:\n    y_pred = model.predict(test, prediction_type='Probability')[:,1] if CATBOOST_FLAG else model.predict(test, num_iteration=model.best_iteration) \n    y_preds.append(y_pred)\n\ny_pred = np.array(y_preds).T.sum(axis=1) / N_SPLITS\npd.DataFrame(pd.Series(y_pred.ravel()).describe()).transpose()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:41:44.846936Z","iopub.execute_input":"2021-11-10T13:41:44.847563Z","iopub.status.idle":"2021-11-10T13:41:45.453568Z","shell.execute_reply.started":"2021-11-10T13:41:44.847518Z","shell.execute_reply":"2021-11-10T13:41:45.452864Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"if not DEBUG_FLAG:\n    submission = pd.read_csv(SUBMISSION_PATH)\n    submission['target'] = y_pred\n    submission.to_csv('submission.csv', index=False)\n    submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:42:28.783267Z","iopub.execute_input":"2021-11-10T13:42:28.783532Z","iopub.status.idle":"2021-11-10T13:42:28.788498Z","shell.execute_reply.started":"2021-11-10T13:42:28.783504Z","shell.execute_reply":"2021-11-10T13:42:28.787389Z"},"trusted":true},"execution_count":62,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n- Kaggle の2021年10月の playground の EDA と予測モデル作成。\n- logging\n- EDA\n    - 基本的な統計量\n    - 考え方：\n        - train_x と train_y の \"相関\"\n        - train_x と test_x の分布の差の検定\n    - target の偏り\n- 特徴量エンジニアリング\n    - 統計量: min, max, mean, std, bin count\n    - 変換： log, power, normalization\n    - 次元削減：k-means, PCA\n    - 集約： `df.groupby(['cat_column']).agg({'continuous_column': ['min', 'max', 'mean', 'std'])\n- ハイパーパラメータ調整\n    - 序盤はハイパラ調整しない。\n    - LightGBM Tuner で自動ハイパラ調整。\n    - 細かくやるなら自前でハイパラ調整。\n- 交差検証\n    - 5-fold CV による予測結果のアンサンブル。\n\nTODO\n- EDA\n    - Theils'U\n- LightGBM\n    - 計算が終わらない >4h\n- XGBoost\n    - [[TPS Oct 21] EDA & Modeling 🔥|Kaggle](https://www.kaggle.com/vishwas21/tps-oct-21-eda-modeling)\n\n# References\n- [Tabular Playground Series - Oct 2021](https://www.kaggle.com/c/tabular-playground-series-oct-2021)\n- [[TPS-Oct] Simple EDA | Kaggle](https://www.kaggle.com/subinium/tps-oct-simple-eda)\n- [TPS Oct 2021: single LightGBM | Kaggle](https://www.kaggle.com/hiro5299834/tps-oct-2021-single-lightgbm)\n- [TPS September 2021 EDA | Kaggle](https://www.kaggle.com/dwin183287/tps-september-2021-eda)\n- スモールサンプル（約7000）[TPS July 2021 EDA | Kaggle](https://www.kaggle.com/dwin183287/tps-july-2021-eda)\n- [Kaggleで多くの実験を回すためにやっている簡単なこと | Zenn](https://zenn.dev/fkubota/articles/2b8d46b11c178ac2fa2d)\n- [kaggle logger introduction | Kaggle](https://www.kaggle.com/sishihara/kaggle-logger-introduction)\n- [Noob Stacking | 0.85654 | Kaggle](https://www.kaggle.com/ankitkalauni/noob-stacking-0-85654/notebook)\n    - stacking に用いる12種の特徴量がほぼ同一の分布であることを確かめている。\n    - それら12列の相関がかなり高い。約0.99\n    - それら12列を使って学習すると、予測値の分布も、それら12列に一致\n    - KDE plot と相関係数の heatmap だけ使ったとのこと\n- [How to Work with Million-row Datasets Like a Pro | towards data science](https://towardsdatascience.com/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd)\n    - 数百万行のデータロードに datatable, メモリ削減の関数、データ操作に datatable, cuDF, Dak, Vaex など。\n    - Cython, numba.jit を使え、CSV よりも feather, parquet, jay を使え、など。","metadata":{}},{"cell_type":"markdown","source":"# Directory","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.751485Z","iopub.execute_input":"2021-11-10T13:19:03.751741Z","iopub.status.idle":"2021-11-10T13:19:03.761110Z","shell.execute_reply.started":"2021-11-10T13:19:03.751712Z","shell.execute_reply":"2021-11-10T13:19:03.760174Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"DEBUG_FLAG = False\nCATBOOST_FLAG = True\nVERSION = 'nb01'\n\nSUBMISSION_PATH = '/kaggle/input/tabular-playground-series-oct-2021/sample_submission.csv'\nTRAIN_PATH = '/kaggle/input/tabular-playground-series-oct-2021/train.csv'\nTEST_PATH = '/kaggle/input/tabular-playground-series-oct-2021/test.csv'\n\nN_SPLITS = 2 if DEBUG_FLAG else 5\nN_ESTIMATORS = 10 if DEBUG_FLAG else 200\nEARLY_STOPPING_ROUNDS = 3 if DEBUG_FLAG else 20","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.762995Z","iopub.execute_input":"2021-11-10T13:19:03.763599Z","iopub.status.idle":"2021-11-10T13:19:03.770307Z","shell.execute_reply.started":"2021-11-10T13:19:03.763562Z","shell.execute_reply":"2021-11-10T13:19:03.769250Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Install & import","metadata":{}},{"cell_type":"code","source":"import datetime\nimport pickle\nimport random\nimport sys\nimport time\n\nimport datatable as dt\nimport lightgbm as lgb\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport scipy.stats as ss\nimport seaborn as sns\n\nfrom catboost import CatBoostClassifier\nfrom contextlib import contextmanager\nfrom logging import getLogger, Formatter, FileHandler, StreamHandler, INFO, DEBUG\n# from optuna.integration import lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.metrics import confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\n\n# settings\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', None)\n\n# mpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T13:19:03.772254Z","iopub.execute_input":"2021-11-10T13:19:03.772805Z","iopub.status.idle":"2021-11-10T13:19:03.782550Z","shell.execute_reply.started":"2021-11-10T13:19:03.772771Z","shell.execute_reply":"2021-11-10T13:19:03.781804Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\n\ndef create_logger(exp_version):\n    log_file = (f'{exp_version}.log')\n\n    logger_ = getLogger(exp_version)\n    logger_.setLevel(DEBUG)\n\n    # formatter\n    # fmr = Formatter('[%(levelname)s] %(asctime)s >>\\t%(message)s')\n    fmr = Formatter(\n        '%(asctime)s %(name)s %(lineno)d'\n        ' [%(levelname)s][%(funcName)s] %(message)s'\n    )\n\n    # file handler\n    fh = FileHandler(log_file)\n    fh.setLevel(DEBUG)\n    fh.setFormatter(fmr)\n\n    # stream handler\n    ch = StreamHandler()\n    ch.setLevel(INFO)\n    ch.setFormatter(fmr)\n\n    logger_.addHandler(fh)\n    logger_.addHandler(ch)\n\n\ndef get_logger(exp_version):\n    return getLogger(exp_version)\n\n\ndef get_args_of_func(f):\n    return f.__code__.co_varnames[:f.__code__.co_argcount]\n\n\ndef show_mem_usage():\n    print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n    print(\" ------------------------------------ \")\n    for var_name in globals():\n        if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 1024**2:\n            print(\"{}{: >25}{}{: >6} MiB{}\".format('|',var_name,'|', int(sys.getsizeof(eval(var_name))/1024**2),'|'))\n\n\ndef read_data(train_only=True):\n    train = dt.fread(TRAIN_PATH).to_pandas()\n    if train_only:\n        test = pd.DataFrame()\n    else:\n        test = dt.fread(TEST_PATH).to_pandas()\n    return train, test\n\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\n\ndef _fe_kmeans(df, n_clusters=6, km=None):\n    cluster_cols = [f'kmeans_{i}' for i in range(n_clusters)]\n    if km is None:\n        km = KMeans(n_clusters=n_clusters, \n                    n_init=50, \n                    max_iter=500, \n                    random_state=0)\n        df_km = km.fit_transform(df)\n    else:\n        df_km = km.transform(df)\n    df_km = pd.DataFrame(df_km, columns=cluster_cols, index=df.index)\n    return df_km, km\n\n\ndef _fe_pca(df, dim=6, pca=None):\n    if pca is None:\n        pca = PCA()\n        df_pca = pca.fit_transform(df[features])\n    else:\n        df_pca = pca.transform(df[features])\n    pca_cols = [f'pca_{i}' for i in range(df_pca.shape[1])]\n    df_pca = pd.DataFrame(df_pca, columns=pca_cols, index=df.index)\n    return df_pca.iloc[:, :dim], pca\n\n\ndef _feature_engineering(df):\n    df['mean'] = df[cont_features].mean(axis=1)\n    df['std'] = df[cont_features].std(axis=1)\n    df['min'] = df[cont_features].min(axis=1)\n    df['max'] = df[cont_features].max(axis=1)\n    df['bin_count'] = df[disc_features].sum(axis=1)\n    return df\n\n\ndef feature_engineering(df, scaler=None, km=None, pca=None):\n    df = _feature_engineering(df)\n    \n    if scaler is None:\n        scaler = RobustScaler()\n        df.loc[:, ['std', 'bin_count']] = scaler.fit_transform(df[['std', 'bin_count']])\n    else:\n        df.loc[:, ['std', 'bin_count']] = scaler.transform(df[['std', 'bin_count']])\n\n    df_km, km = _fe_kmeans(df[features], km=km)\n    df_pca, pca = _fe_pca(df[features], pca=pca)\n    df = pd.concat([df, df_km, df_pca], axis=1)\n    \n    return df, scaler, km, pca","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.785194Z","iopub.execute_input":"2021-11-10T13:19:03.785492Z","iopub.status.idle":"2021-11-10T13:19:03.828815Z","shell.execute_reply.started":"2021-11-10T13:19:03.785467Z","shell.execute_reply":"2021-11-10T13:19:03.828097Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Preparing","metadata":{}},{"cell_type":"code","source":"create_logger(VERSION)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.933495Z","iopub.execute_input":"2021-11-10T13:19:03.933821Z","iopub.status.idle":"2021-11-10T13:19:03.938809Z","shell.execute_reply.started":"2021-11-10T13:19:03.933796Z","shell.execute_reply":"2021-11-10T13:19:03.937845Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Dataset overview","metadata":{}},{"cell_type":"code","source":"train, test = read_data(train_only=False)\n\nif not DEBUG_FLAG:\n    train = reduce_memory_usage(train)\n    test = reduce_memory_usage(test) \n\nif DEBUG_FLAG:\n    train = train.sample(n=10000)\n    if len(test) != 0:\n        test = test.sample(n=10000)\n\n    \nget_logger(VERSION).info(f'train.shape: {train.shape}')\nget_logger(VERSION).info(f'test.shape: {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:03.940902Z","iopub.execute_input":"2021-11-10T13:19:03.941925Z","iopub.status.idle":"2021-11-10T13:19:15.486401Z","shell.execute_reply.started":"2021-11-10T13:19:03.941890Z","shell.execute_reply":"2021-11-10T13:19:15.485697Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"get_logger(VERSION).info('train data')\nget_logger(VERSION).info(train.head(1))\n\nget_logger(VERSION).info('test data')\nget_logger(VERSION).info(test.head(1))\n\nget_logger(VERSION).info(f'number of missing values in train: {train.isnull().sum().sum()}')\nget_logger(VERSION).info(f'number of missing values in test: {test.isnull().sum().sum()}')\n\ntarget_count = train['target'].value_counts()\nget_logger(VERSION).info(f'target count below:')\nget_logger(VERSION).info(target_count)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:15.487685Z","iopub.execute_input":"2021-11-10T13:19:15.488369Z","iopub.status.idle":"2021-11-10T13:19:16.278000Z","shell.execute_reply.started":"2021-11-10T13:19:15.488342Z","shell.execute_reply":"2021-11-10T13:19:16.277264Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    plt.pie(target_count, \n        labels=target_count.index,\n        colors=['#76D7C4', '#F5B7B1'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.279280Z","iopub.execute_input":"2021-11-10T13:19:16.279672Z","iopub.status.idle":"2021-11-10T13:19:16.353418Z","shell.execute_reply.started":"2021-11-10T13:19:16.279638Z","shell.execute_reply":"2021-11-10T13:19:16.352765Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    display(train.loc[:, 'f0':'f284'].describe().T.style.bar(subset=['mean'], color='#205ff2') \\\n        .background_gradient(subset=['std'], cmap='Reds') \\\n        .background_gradient(subset=['50%'], cmap='coolwarm'))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.355501Z","iopub.execute_input":"2021-11-10T13:19:16.356177Z","iopub.status.idle":"2021-11-10T13:19:16.983045Z","shell.execute_reply.started":"2021-11-10T13:19:16.356141Z","shell.execute_reply":"2021-11-10T13:19:16.982262Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"features = [col for col in train.columns if col not in ['id', 'target']]\n\ncont_features = []\ndisc_features = []\n\nfor col in features:\n    if np.issubdtype(train[col].dtype, np.floating):\n        cont_features.append(col)\n    else:\n        disc_features.append(col)\n\nget_logger(VERSION).info('cont. features below:')\nget_logger(VERSION).info(cont_features)\nget_logger(VERSION).info('disc. features below:')\nget_logger(VERSION).info(disc_features)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:16.984597Z","iopub.execute_input":"2021-11-10T13:19:16.985070Z","iopub.status.idle":"2021-11-10T13:19:17.022949Z","shell.execute_reply.started":"2021-11-10T13:19:16.985035Z","shell.execute_reply":"2021-11-10T13:19:17.022152Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Correlation\n\n||continuous data|ordinal data|categorical data|\n|-:|:-|:-|:-|\n|continuous data|Pearson correlation coefficient|polyserial correlation coefficient|correlation ratio|\n|ordinal data||Spearman's rank correlation coefficient, <br>Kendall rank correlation coefficient, <br>Polychoric correlation coefficient|(rank correlation ratio?)|\n|categorical data|||four-fold point correlation coefficient (phi coefficient), <br>Cramér's V, <br>Uncertainty coefficient (Theil's U)|\n\n[Ref.]\n- [5.5 各種手法の相互関係 | 統計学入門](http://www.snap-tck.com/room04/c01/stat/stat05/stat0505.html)","metadata":{}},{"cell_type":"code","source":"if DEBUG_FLAG:\n    cramers_v_scores = []\n    for f in disc_features:\n        cramers_v_scores.append((f, cramers_v(train['target'], train[f])))\n\n    display(pd.DataFrame(cramers_v_scores, columns=['disc_feature', 'cramers_v']) \\\n        .sort_values('cramers_v', ascending=False) \\\n        .style.bar(subset=['cramers_v'], color='#205ff2')\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:17.024471Z","iopub.execute_input":"2021-11-10T13:19:17.024972Z","iopub.status.idle":"2021-11-10T13:19:17.606411Z","shell.execute_reply.started":"2021-11-10T13:19:17.024932Z","shell.execute_reply":"2021-11-10T13:19:17.605614Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    plt.rcParams['font.size'] = 8\n    fig, axes = plt.subplots(nrows=29, ncols=10, tight_layout=True, figsize=(18, 36))\n\n    for i, ax in enumerate(axes.ravel()):\n        if i < len(features):\n            if f'f{i}' in cont_features:\n                sns.kdeplot(train[f'f{i}'], lw=.5, alpha=.3, shade=True, ax=ax)\n                sns.kdeplot(test[f'f{i}'], lw=.5, alpha=.3, shade=True, ax=ax)\n            elif f'f{i}' in disc_features:\n                tr = train[[f'f{i}']]\n                tr = tr.assign(data = 'Train')\n                te = test[[f'f{i}']]\n                te = te.assign(data = 'Test')\n                sns.countplot(x='data', \n                              hue=f'f{i}', \n                              order=['Train', 'Test'],\n                              alpha=.3, \n                              data=pd.concat([tr, te]), \n                              ax=ax)\n        else:\n            ax.set_axis_off()\n        \n        ax.set_ylabel(None)\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:19:17.607817Z","iopub.execute_input":"2021-11-10T13:19:17.608682Z","iopub.status.idle":"2021-11-10T13:20:08.920319Z","shell.execute_reply.started":"2021-11-10T13:19:17.608638Z","shell.execute_reply":"2021-11-10T13:20:08.919657Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 15))\n    sns.heatmap(train[cont_features].corr(), \n                vmin=-1.0,\n                vmax=1.0,\n                annot=True, \n                fmt='1.1f', \n                cmap='coolwarm', \n                ax=ax)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:20:08.921475Z","iopub.execute_input":"2021-11-10T13:20:08.922189Z","iopub.status.idle":"2021-11-10T13:23:08.444719Z","shell.execute_reply.started":"2021-11-10T13:20:08.922153Z","shell.execute_reply":"2021-11-10T13:23:08.444109Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## 相互情報量\nそれぞれの特徴量の、ターゲット予測因子としての有用性を確認する。  \nいくつか方法はあるが、ひとつの方法として、相互情報量を計算する。  \n相互情報量は、相関係数のようにふたつの特徴量間の \"相関関係\" を定量化する。  \n相関係数が線形な関係に注目して定量化するのに対し、相互情報量は線形でなくても相関関係を定量化できる。  \n相互情報量の利点は以下のとおり：\n- 簡単に使えるし、簡単に説明できる\n- 効率的に計算できる\n- 理論的な根拠がしっかりしている\n- 過学習に耐性がある（本当か？）\n- 任意の種類の関係を検出できる\n\n2つの特徴量に対する相互情報量は、一方の特徴量の値を知ることで、他方の値の不確実性がどれだけ低減するかを示す尺度である。  \n相互情報量の最小の値は0で、この場合は2つの特徴量が独立な関係にあることを意味する。  \n相互情報量の上限はない。\n\n相互情報量は、その特徴量単独で予測因子としたときの有用性を確認することができる。  \n複数の特徴量の相互作用によって、それらの特徴量がターゲットの予測因子として有用であるが、それぞれ単独では有用ではないことがあるので、相互情報量に意味がある。\n\n<br><br>\n定義と例を確認する。  \nふたつの離散確率変数XとYの相互情報量は以下のように定義される：\n$$\nI(X, Y)=\\sum_{y\\in Y}\\sum_{x\\in X}p(x, y)\\log\\frac{p(x, y)}{p(x)p(y)}.\n$$\nここで$p(x, y)$は$X$と$Y$の同時分布関数、$p(x)$と$p(y)$はそれぞれ$X$と$Y$の周辺分布関数である。\n\n以下は確率変数XとYが、どちらも取りうる値が$\\{0, 1\\}$の場合のクロス表。\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |$a_{00}$|$a_{01}$|\n| X=1  |$a_{10}$|$a_{11}$|\n\n総数を$N (=a_{00}+a_{01}+a_{10}+a_{11})$とすると、$p(X=0, Y=0)=a_{00}/N$, $p(X=0)=(a_{00}+a_{01})/N$, $p(Y=0)=(a_{00}+a_{10})/N$となる。  \nこのときの相互情報量は\n$$\n\\begin{align}\nI(X, Y)\n= & p(X=0, Y=0)\\log\\frac{p(X=0, Y=0)}{p(X=0)p(Y=0)} \\\\\n+ & p(X=1, Y=0)\\log\\frac{p(X=1, Y=0)}{p(X=1)p(Y=0)} \\\\\n+ & p(X=0, Y=1)\\log\\frac{p(X=0, Y=1)}{p(X=0)p(Y=1)} \\\\\n+ & p(X=1, Y=1)\\log\\frac{p(X=1, Y=1)}{p(X=1)p(Y=1)} \\\\[10pt]\n= & \\frac{a_{00}}{N}\\log\\frac{a_{00}/N}{(a_{00}+a_{01})/N\\cdot (a_{00}+a_{10})/N} \\\\\n+ & \\frac{a_{10}}{N}\\log\\frac{a_{10}/N}{(a_{10}+a_{11})/N\\cdot (a_{00}+a_{10})/N} \\\\\n+ & \\frac{a_{01}}{N}\\log\\frac{a_{01}/N}{(a_{00}+a_{01})/N\\cdot (a_{01}+a_{11})/N} \\\\\n+ & \\frac{a_{11}}{N}\\log\\frac{a_{11}/N}{(a_{10}+a_{11})/N\\cdot (a_{01}+a_{11})/N}\n\\end{align}\n$$\n\n極端な例を考える。 \n\n#### 例1\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |   1  |\n\nこのときの相互情報量は\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{2}\\log\\frac{1/2}{1/2\\cdot 1/2}\n+  \\frac{0}{2}\\log\\frac{0/2}{1/2\\cdot 1/2}\n+  \\frac{0}{2}\\log\\frac{0/2}{1/2\\cdot 1/2}\n+  \\frac{1}{2}\\log\\frac{1/2}{1/2\\cdot 1/2} \\\\[10pt]\n=& 1\n\\end{align}\n$$\n\n#### 例2\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   1  |\n| X=1  |   1  |   1  |\n\nこのときの相互情報量は\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4}\n+  \\frac{1}{4}\\log\\frac{1/4}{2/4\\cdot 2/4} \\\\[10pt]\n=& 0\n\\end{align}\n$$\n\n#### 例3\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |   3  |\n\nこのときの相互情報量は\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{4}\\log\\frac{1/4}{1/4\\cdot 1/4}\n+  \\frac{0}{4}\\log\\frac{0/4}{3/4\\cdot 1/4}\n+  \\frac{0}{4}\\log\\frac{0/4}{1/4\\cdot 3/4}\n+  \\frac{3}{4}\\log\\frac{3/4}{3/4\\cdot 3/4} \\\\[10pt]\n=& \\frac{1}{4}\\log 4 + \\frac{3}{4}\\log \\frac{4}{3} \\\\[10pt]\n\\approx& 0.8112\n\\end{align}\n$$\n\n#### 例4\n\n|      | Y=0  | Y=1  |\n| ---- | ---- | ---- |\n| X=0  |   1  |   0  |\n| X=1  |   0  |  99  |\n\nこのときの相互情報量は\n\n$$\n\\begin{align}\nI(X, Y)\n=& \\frac{1}{100}\\log\\frac{1/100}{1/100\\cdot 1/100}\n+  \\frac{0}{100}\\log\\frac{0/100}{99/100\\cdot 1/100}\n+  \\frac{0}{100}\\log\\frac{0/100}{1/100\\cdot 99/100}\n+  \\frac{99}{100}\\log\\frac{99/100}{99/100\\cdot 99/100} \\\\[10pt]\n=& \\frac{1}{100}\\log 100 + \\frac{99}{100}\\log \\frac{100}{99} \\\\[10pt]\n\\approx& 0.0808\n\\end{align}\n$$\n\n<br><br>\nscikit-learn では、ターゲットが離散データか量的データかで、使う関数が異なる。  \n離散の場合は sklearn.feature_selection.mutual_info_refression である。  \n連続の場合は sklearn.feature_selection.mutual_info_classif である。\n\n[参考記事] \n- [Mutual Information | Kaggle](https://www.kaggle.com/ryanholbrook/mutual-information)\n- クロス表から計算 [相互情報量を用いた特徴選択 | 人工知能に関する断創録](https://aidiary.hatenablog.com/entry/20100619/1276950312)","metadata":{}},{"cell_type":"code","source":"if DEBUG_FLAG:\n    mi_scores = mutual_info_classif(train[features], train['target'], discrete_features=[f in disc_features for f in features])\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=features)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    \n    def plot_mi_scores(scores):\n        scores = scores.sort_values(ascending=True)\n        width = np.arange(len(scores))\n        ticks = list(scores.index)\n        plt.barh(width, scores, height=.4)\n        plt.yticks(width, ticks)\n        plt.ylim(-1, len(scores))\n        plt.title('Mutual Information Scores')\n        plt.grid()\n\n    top = 40\n    # plt.style.use('seaborn-talk')\n    plt.figure(figsize=(8, 8))\n    plot_mi_scores(mi_scores[:top])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:08.445931Z","iopub.execute_input":"2021-11-10T13:23:08.446307Z","iopub.status.idle":"2021-11-10T13:23:28.681629Z","shell.execute_reply.started":"2021-11-10T13:23:08.446239Z","shell.execute_reply":"2021-11-10T13:23:28.680957Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"show_mem_usage()\n\nif not DEBUG_FLAG and 'test' in globals():\n    del test","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.682929Z","iopub.execute_input":"2021-11-10T13:23:28.683324Z","iopub.status.idle":"2021-11-10T13:23:28.747760Z","shell.execute_reply.started":"2021-11-10T13:23:28.683287Z","shell.execute_reply":"2021-11-10T13:23:28.746996Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Model build\n\n予測モデルを構築する。\nここでは勾配ブースティングのひとつであるLightGBMを用いる。  \n勾配ブースティングは、弱学習器をつくり、その誤差を吸収するように弱学習器を逐次的につくる学習フレームワークである。  \n弱学習器には、決定木が用いられることが多く、Gradient Boosting Decision Tree (GBDT) という。  \nGBDTのなかでは、XGBoost, LightGBM, CatBoostなどが有名である。  \nそれらはどう異なるのか、pros, consについて調査する。\n\n**XGBoost**  \nXGBoostは、分割する特徴量を選択する際に、事前ソートによるアルゴリズムと、ヒストグラムによるアルゴリズムの2種類を使う。  \n事前ソートとは、分割の際に、任意の特徴量の値に基づいてサンプルをソートし、その特徴量で分割する際の情報利得による最適な分割値を決定する。  \nこれにより、すべての特徴量それぞれで分割する際の最適な分割値を得る。  \nヒストグラムベースのアルゴリズムでは、それぞれの特徴量でヒストグラムを構成し、そのビンを使って分割値を求める。  \nこれら2つの分割アルゴリズムは、速度の点でヒストグラムベースのアルゴリズムが優れている。\n\n**LightGBM**  \nLightGBMは、分割する特徴量を選択する際に、Gradient-based One-Side Sampling (GOSS)を用いる。  \nここで \"勾配\" が意味しているのは、損失関数に対する接線の傾きである。■■■接線とは？要確認  \nサンプルごとに勾配を計算し、勾配が大きいサンプルはそのまま残し、勾配が小さいサンプルはランダムにサンプリングする。  \n\n**CatBoost**  \nカテゴリ特徴量の扱い方に特徴がある。  \n特徴量の一意な値の数が、 \"one_hot_max_size\" で指定した値以下のときに one hot encoding を行う。  \n特徴量の一意な値の数が、 \"one_hot_max_size\" で指定した値よりも大きいとき Ordered TS と呼ばれる Target Encoding の処理が行われる。\n\nちなみに、XGBoost は LightGBM や CatBoost とは異なり、カテゴリ特徴量を内部的に扱うことはできず、人手でエンコーディングする必要がある。\n\n|Func.|XGBoost|CatBoost|LightGBM|\n|-:|:-|:-|:-|\n|control overfitting|learning rate or eta, <br>max_depth, <br> min_child_weight|learning_rate, <br>depth, <br>(no such feature like min_child_weight)|learning_rate, <br>max_depth, <br>num_leaves, <br>min_data_in_leaf|\n|categoriccal values|(not available)|cat_features, <br>one_hot_max_size|categorical_feature|\n|controlling speed|colsample_bytree, <br>subsample, <br>n_estimators|rsm, <br>(no such parameter to subset data), <br>iteratons|feature_fraction, <br>bagging_fraction, <br>num_iterations|\n\n[参考]\n- [勾配ブースティングの基礎と最新の動向 (MIRU2020 Tutorial)](https://www.slideshare.net/RyuichiKanoh/miru2020-tutorial-237272385)\n- [CatBoost vs. Light GBM vs. XGBoost | towards data science](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)","metadata":{}},{"cell_type":"code","source":"?lgb.train","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.748858Z","iopub.execute_input":"2021-11-10T13:23:28.749399Z","iopub.status.idle":"2021-11-10T13:23:28.760014Z","shell.execute_reply.started":"2021-11-10T13:23:28.749359Z","shell.execute_reply":"2021-11-10T13:23:28.759206Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"models = []\nresults = []\noof_train = np.zeros((len(train),))\n\nstkf = StratifiedKFold(n_splits=N_SPLITS, \n                       shuffle=True, \n                       random_state=0)\n\nparams_lgb = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'max_depth': 12,\n    #'num_leaves': 500,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.80,\n    'bagging_fraction': 0.80,\n    'bagging_freq': 5,\n    'max_bin': 100,\n    'n_jobs':-1,\n    'verbose': -1,\n    #'num_threads': 1,\n    'lambda_l2': 1.5,\n    #'min_gain_to_split': 0,\n    # 'is_unbalance': True,\n    #'scale_pos_weight':0.15,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}  \nparams_lgb = {\n    'objective': 'binary',\n    'n_estimators': 2000,\n    'random_state': 0,\n    'learning_rate': 0.05,\n    'early_stopping_rounds': 10,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device': 'gpu',\n}\n\n\nparams_cat = {\n    'iterations': 2000, \n    'loss_function': 'Logloss', \n    'depth': 8, \n    'task_type' : 'GPU',\n    'use_best_model': True,\n    'eval_metric': 'AUC',\n    'early_stopping_rounds': 100,\n    'learning_rate': 0.05,\n    'border_count': 32,\n    'l2_leaf_reg': 3,\n    'verbose': 100\n}\n\ny_train = train['target']\nX_train = train.drop(['id', 'target'], axis=1)\n\nget_logger(VERSION).info(f'cross validation:')\n\nfor fold_id, (train_idx, valid_idx) in enumerate(stkf.split(X_train, y_train)):\n    start = time.time()\n    result = {}\n    \n    get_logger(VERSION).info(f'||' * 40)\n    get_logger(VERSION).info(f'fold_id: {fold_id}')\n    \n    X_tr = X_train.iloc[train_idx, :].reset_index(drop=True)\n    X_val = X_train.iloc[valid_idx, :].reset_index(drop=True)\n    y_tr = y_train.iloc[train_idx].reset_index(drop=True)\n    y_val = y_train.iloc[valid_idx].reset_index(drop=True)\n    \n    get_logger(VERSION).info(f'feature engineering:')\n    \n    #X_tr, scaler, km, pca = feature_engineering(X_tr)\n    #X_val, scaler, km, pca = feature_engineering(X_val, scaler, km, pca)\n    \n    get_logger(VERSION).info(' '.join(X_tr.columns))\n    \n    if CATBOOST_FLAG:\n        model_cat = CatBoostClassifier(**params_cat)\n        model_cat.fit(X_tr,\n                      y_tr,\n                      eval_set=[(X_val, y_val)],\n                      early_stopping_rounds=10)\n        \n        oof_train[valid_idx] = model_cat.predict_proba(X_val)[:,1]\n        models.append(model_cat)\n        \n    else:\n        lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=disc_features)\n        lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature=disc_features)\n\n        model_lgb = lgb.train(params_lgb,\n                              lgb_train,\n                              valid_sets=[lgb_train, lgb_eval],\n                              valid_names=['Train', 'Valid'],\n                              verbose_eval=100,\n                              num_boost_round=500,\n                              evals_result=result)\n\n        oof_train[valid_idx] = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n        models.append(model_lgb)\n        results.append(result)\n    \n    score_auc = roc_auc_score(y_val, oof_train[valid_idx])\n    elapsed = time.time() - start\n    get_logger(VERSION).info(f'fold {fold_id} - auc: {score_auc:.6f}, elapsed time: {elapsed:.2f} [sec]\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:28.761619Z","iopub.execute_input":"2021-11-10T13:23:28.761894Z","iopub.status.idle":"2021-11-10T13:23:42.942930Z","shell.execute_reply.started":"2021-11-10T13:23:28.761861Z","shell.execute_reply":"2021-11-10T13:23:42.942281Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nmodel_name = f'trained_model_lgb_{now}.pkl'\npickle.dump(models, open(model_name, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:42.948531Z","iopub.execute_input":"2021-11-10T13:23:42.949478Z","iopub.status.idle":"2021-11-10T13:23:42.963897Z","shell.execute_reply.started":"2021-11-10T13:23:42.949439Z","shell.execute_reply":"2021-11-10T13:23:42.963266Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"if CATBOOST_FLAG:\n    fig, axes = plt.subplots(nrows=1, ncols=len(models), tight_layout=True, figsize=(6*len(models), 4))\n    for i, model in enumerate(models):\n        history = model.get_evals_result()\n        train_metric = history['learn']['Logloss']\n        axes[i].plot(train_metric, label='train metric')\n        eval_metric = history['validation']['Logloss']\n        axes[i].plot(eval_metric, label='eval metric')\n        axes[i].set_title(f'fold {i}')\n        axes[i].set_xlabel('Iterations')\n        axes[i].set_ylabel('logloss')\nelse:\n    fig, axes = plt.subplots(nrows=1, ncols=len(results), tight_layout=True, figsize=(6*len(results), 4))\n    for i, res in enumerate(results):\n        lgb.plot_metric(res, ax=axes[i])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:23:42.968013Z","iopub.execute_input":"2021-11-10T13:23:42.970516Z","iopub.status.idle":"2021-11-10T13:23:43.403648Z","shell.execute_reply.started":"2021-11-10T13:23:42.970482Z","shell.execute_reply":"2021-11-10T13:23:43.403099Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"test = dt.fread(TEST_PATH).to_pandas()\ntest = test.drop(['id'], axis=1)\nif DEBUG_FLAG:\n    test = test.sample(n=10000)\n#test, _, _, _ = feature_engineering(test, scaler, km, pca)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:41:39.090280Z","iopub.execute_input":"2021-11-10T13:41:39.090563Z","iopub.status.idle":"2021-11-10T13:41:42.775156Z","shell.execute_reply.started":"2021-11-10T13:41:39.090527Z","shell.execute_reply":"2021-11-10T13:41:42.774271Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"models = pickle.load(open(model_name, 'rb'))\n\ny_preds = []\nfor model in models:\n    y_pred = model.predict(test, prediction_type='Probability')[:,1] if CATBOOST_FLAG else model.predict(test, num_iteration=model.best_iteration) \n    y_preds.append(y_pred)\n\ny_pred = np.array(y_preds).T.sum(axis=1) / N_SPLITS\npd.DataFrame(pd.Series(y_pred.ravel()).describe()).transpose()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:41:44.846936Z","iopub.execute_input":"2021-11-10T13:41:44.847563Z","iopub.status.idle":"2021-11-10T13:41:45.453568Z","shell.execute_reply.started":"2021-11-10T13:41:44.847518Z","shell.execute_reply":"2021-11-10T13:41:45.452864Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"if not DEBUG_FLAG:\n    submission = pd.read_csv(SUBMISSION_PATH)\n    submission['target'] = y_pred\n    submission.to_csv('submission.csv', index=False)\n    submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T13:42:28.783267Z","iopub.execute_input":"2021-11-10T13:42:28.783532Z","iopub.status.idle":"2021-11-10T13:42:28.788498Z","shell.execute_reply.started":"2021-11-10T13:42:28.783504Z","shell.execute_reply":"2021-11-10T13:42:28.787389Z"},"trusted":true},"execution_count":62,"outputs":[]}]}